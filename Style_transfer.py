# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/167LooPXIgNqMjw94IFLNKsSj2xY-So-n
"""

import urllib.request 
url = 'http://images.cocodataset.org/zips/train2014.zip'  
urllib.request.urlretrieve(url, '/content/data_set.zip')

import zipfile
with zipfile.ZipFile('/content/data_set.zip', 'r') as zip_ref:
    zip_ref.extractall('/content/training_data')

WEIGHTS_PATH_NO_TOP = 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'
from keras.utils.data_utils import get_file
weights_path = get_file('vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',
                                    WEIGHTS_PATH_NO_TOP,
                                    cache_subdir='models')

from os import listdir
from os.path import isfile, join
files = [f for f in listdir('/content/training_data/train2014') if isfile(join('/content/training_data/train2014', f))]

from keras import layers
from keras.models import Model
import keras.backend as k
import tensorflow as tf
from keras.engine.topology import Layer
from keras.regularizers import Regularizer

img_width,img_height = (512,512)

from keras.preprocessing import image
import numpy as np
def preprocess_image(image_path,rows,cols):
    img = image.load_img(image_path, target_size=(rows, cols))
    img = image.img_to_array(img)
    img = np.expand_dims(img, axis=0)
    return img

from keras.engine.topology import Layer
class Denormalize(Layer):


    def __init__(self, **kwargs):
        super(Denormalize, self).__init__(**kwargs)

    def build(self, input_shape):
        pass

    def call(self, x, mask=None):

        return (x + 1) * 127.5
      
class InputNormalize(Layer):
    def __init__(self, **kwargs):
        super(InputNormalize, self).__init__(**kwargs)

    def build(self, input_shape):
        pass

    def compute_output_shape(self,input_shape):
        return input_shape

    def call(self, x, mask=None):
        return x/255.
      
class VGGNormalize(Layer):


    def __init__(self, **kwargs):
        super(VGGNormalize, self).__init__(**kwargs)

    def build(self, input_shape):
        pass

    def call(self, x, mask=None):

        x = x[:, :, :, ::-1]       
        x -= 120
        
        return x

def transform_net(img_width,img_height):
  input_tensor = layers.Input(shape=(img_width,img_height,3))
  input_tensor1 = InputNormalize()(input_tensor)
  x = layers.Conv2D(32, kernel_size = (9,9), strides = (1,1), padding = 'same')(input_tensor1)
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  
  x = layers.Conv2D(64, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  
  x = layers.Conv2D(128, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  
  x = residual_block(x)
  x = residual_block(x)
  x = residual_block(x)
  x = residual_block(x)
  x = residual_block(x)
  x = layers.Conv2DTranspose(64, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  
  x = layers.Conv2DTranspose(32, kernel_size = (3,3), strides = (2,2), padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  
  x = layers.Conv2DTranspose(3, kernel_size = (9,9), strides = (1,1), padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  output_tensor = layers.Activation('tanh')(x)  
  output_tensor2 = Denormalize()(output_tensor)
  model = Model(inputs = input_tensor,outputs = output_tensor2)  
  return model

def residual_block(x):
  y = x
  x = layers.Conv2D(128,kernel_size = (3,3),strides = (1,1),padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  x = layers.Activation('relu')(x)
  x = layers.Conv2D(128,kernel_size = (3,3),strides = (1,1),padding = 'same')(x)
  x = layers.BatchNormalization()(x)
  res = layers.merge.add([x, y])
  return res

"""# Losses"""

from keras import backend as K
from keras.regularizers import Regularizer
from keras.objectives import mean_squared_error


def dummy_loss(y_true, y_pred ):
    return K.variable(0.0)

def gram_matrix(x):
    assert K.ndim(x) == 3
    if K.image_dim_ordering() == 'th':
        features = K.batch_flatten(x)
    else:
        features = K.batch_flatten(K.permute_dimensions(x, (2, 0, 1)))
    
    shape = K.shape(x)
    
    C, W, H = (shape[0],shape[1], shape[2])
    
    cf = K.reshape(features ,(C,-1))
    gram = K.dot(cf, K.transpose(cf)) /  K.cast(C*W*H,dtype='float32')

    return gram


class StyleReconstructionRegularizer(Regularizer):

    def __init__(self, style_feature_target, weight=1.0):
        self.style_feature_target = style_feature_target
        self.weight = weight
        self.uses_learning_phase = False
        super(StyleReconstructionRegularizer, self).__init__()

        self.style_gram = gram_matrix(style_feature_target)

    def __call__(self, x):
        output = x.output[0]
        loss = self.weight *  K.sum(K.mean(K.square((self.style_gram-gram_matrix(output) )))) 

        return loss


class FeatureReconstructionRegularizer(Regularizer):
    def __init__(self, weight=1.0):
        self.weight = weight
        self.uses_learning_phase = False
        super(FeatureReconstructionRegularizer, self).__init__()

    def __call__(self, x):
        generated = x.output[0] 
        content = x.output[1] 

        loss = self.weight *  K.sum(K.mean(K.square(content-generated)))
        return loss
      
class TVRegularizer(Regularizer):

    def __init__(self, weight=1.0):
        self.weight = weight
        self.uses_learning_phase = False
        super(TVRegularizer, self).__init__()

    def __call__(self, x):
        assert K.ndim(x.output) == 4
        x_out = x.output
        
        shape = K.shape(x_out)
        img_width, img_height,channel = (shape[1],shape[2], shape[3])
        size = img_width * img_height * channel 
        a = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, 1:, :img_height - 1, :])
        b = K.square(x_out[:, :img_width - 1, :img_height - 1, :] - x_out[:, :img_width - 1, 1:, :])
        loss = self.weight * K.sum(K.pow(a + b, 1.25)) 
        return loss

def add_style_loss(vgg,style_image_path,vgg_layers,vgg_output_dict,img_width, img_height,weight):
    style_img = preprocess_image(style_image_path, img_width, img_height)
    style_layers = ['block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1','block5_conv1']

    style_layer_outputs = []

    for layer in style_layers:
        style_layer_outputs.append(vgg_output_dict[layer])

    vgg_style_func = K.function([vgg.layers[-19].input], style_layer_outputs)

    style_features = vgg_style_func([style_img])

    for i, layer_name in enumerate(style_layers):
        layer = vgg_layers[layer_name]

        feature_var = K.variable(value=style_features[i][0])
        style_loss = StyleReconstructionRegularizer(
                            style_feature_target=feature_var,
                            weight=weight)(layer)

        layer.add_loss(style_loss)
        
def add_content_loss(vgg_layers,vgg_output_dict,weight):

    content_layer = 'block4_conv2'
    content_layer_output = vgg_output_dict[content_layer]

    layer = vgg_layers[content_layer]
    content_regularizer = FeatureReconstructionRegularizer(weight)(layer)
    layer.add_loss(content_regularizer)
def add_total_variation_loss(transform_output_layer,weight):
    layer = transform_output_layer 
    tv_regularizer = TVRegularizer(weight)(layer)
    layer.add_loss(tv_regularizer)



from keras.applications.vgg16 import VGG16
from keras.applications.vgg19 import VGG19
transformer = transform_net(img_width,img_height)
tensor1 = layers.merge.concatenate([transformer.output,transformer.input],axis = 0)
tensor2 = VGGNormalize(name="vgg_normalize")(tensor1)
vgg1 = VGG16(include_top = False,input_tensor = tensor2,weights = None)

vgg1.load_weights(weights_path,by_name = True)

vgg_output_dict = dict([(layer.name, layer.output) for layer in vgg1.layers[-18:]])
vgg_layers = dict([(layer.name, layer) for layer in vgg1.layers[-18:]])
add_style_loss(vgg1,'/content/style.jpg' , vgg_layers, vgg_output_dict, img_width, img_height,3)   
add_content_loss(vgg_layers,vgg_output_dict,2)
add_total_variation_loss(transformer.layers[-1],0.000005)

for layer in vgg1.layers[-19:]:
  layer.trainable = False

def dummy_loss(y_true, y_pred ):
    return k.variable(0.0)

import keras
optimizer = keras.optimizers.Adam()
vgg1.compile(optimizer,loss=dummy_loss)

from keras.preprocessing.image import ImageDataGenerator
datagen = ImageDataGenerator()
train_data = datagen.flow_from_directory('/content/training_data',batch_size = 1,target_size = (img_width,img_height),class_mode = 'input')

history= vgg1.fit_generator(
train_data,
steps_per_epoch=83000,
epochs=1)

